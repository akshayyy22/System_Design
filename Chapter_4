Chapter 4: Encoding and Evolution 

This chapter discusses how data is encoded (serialized) for storage and transmission, and how data formats evolve over time as applications change. It covers different serialization formats, schema evolution strategies, and trade-offs in compatibility.  

---

### Summary

- Why Encoding Matters  
  - Data must be stored and transmitted in a structured format.  
  - Different formats have different trade-offs in terms of performance, readability, and compatibility.  

- Serialization Formats  
  - Language-Specific Formats – Java’s `Serializable`, Python’s `pickle`, etc. (not portable).  
  - Text-Based Formats – JSON, XML, CSV (human-readable but inefficient).  
  - Binary Formats – Protocol Buffers (Protobuf), Apache Avro, Apache Thrift (compact, schema-based).  

- Schema Evolution 
  - Forward Compatibility – New software can read old data.  
  - Backward Compatibility – Old software can read new data.  
  - Bidirectional Compatibility – Data remains readable in both directions.  

- Dataflow and Storage 
  - Databases store structured data, requiring careful schema design.  
  - Data exchange happens via REST, RPC, and message queues, requiring compatibility handling.  

- Message-Passing and Streaming  
  - Systems like Kafka, gRPC, and REST APIs need a consistent encoding format to avoid breaking changes.  

---

## Key Concepts 
1. Data Encoding Types  
   - Human-Readable Formats – JSON, XML, CSV.  
   - Compact Binary Formats – Avro, Protobuf, Thrift.  
   - Language-Specific Serialization – Java `Serializable`, Python `pickle`.  

2. Schema Evolution Strategies  
   - Forward Compatibility – New software reads old data.  
   - Backward Compatibility – Old software reads new data.  
   - Schema Migration – Updating database schemas safely.  

3. Trade-Offs Between Formats 
   - Performance vs. Readability – JSON is human-readable but slower, Protobuf is compact but requires schema.  
   - Self-Describing vs. External Schema – JSON has inline structure, Avro requires a predefined schema.  

4. Modes of Dataflow  
   - REST APIs (JSON, XML) – Stateless communication.  
   - gRPC & Thrift (Protobuf, Binary) – Efficient RPC-based communication.  
   - Message Queues (Kafka, RabbitMQ) – Durable message storage and delivery.  

---

## Keywords
- Serialization, Deserialization  
- JSON, XML, CSV, Protobuf, Avro, Thrift 
- Backward & Forward Compatibility  
- Schema Evolution, Schema Migration  
- Message-Passing, RPC, REST, gRPC  
- Compact Binary Encoding  
- Dataflow: Databases, APIs, Streaming 

---

## Interview Questions for Google & Amazon (SDE Intern)  

### Conceptual Questions
1. What are the differences between JSON, Protobuf, and Avro?  
2. Why are binary formats like Protobuf and Avro preferred over JSON for large-scale applications? 
3. What is schema evolution, and why is it important?  
4. How do you ensure backward and forward compatibility in data serialization?  
5. What are the trade-offs between language-specific and language-agnostic serialization formats?  
6. How does gRPC differ from REST, and why does it use Protobuf?  
7. What challenges arise when migrating database schemas?  
8. How does Kafka handle schema evolution in message streams?  

### System Design & Practical Questions
9. How would you design a system that transmits structured data between microservices efficiently?  
10. What serialization format would you choose for an IoT system with limited bandwidth?
11. If you were designing a distributed database, how would you handle schema evolution?
12. How do you migrate a REST API from JSON to Protobuf while ensuring backward compatibility?  
13. How would you handle a scenario where two services expect different versions of a data schema?  
14. What are some strategies to update a large database schema with minimal downtime?  

---
Summary

In this chapter we looked at several ways of turning data structures into bytes on the network or bytes on disk. We saw how the details of these encodings affect not only their efficiency, but more importantly also the architecture of applications and your options for deploying them.
In particular, many services need to support rolling upgrades, where a new version of a service is gradually deployed to a few nodes at a time, rather than deploying to all nodes simultaneously. Rolling upgrades allow new versions of a service to be released without downtime (thus encouraging frequent small releases over rare big releases) and make deployments less risky (allowing faulty releases to be detected and rolled back before they affect a large number of users). These properties are hugely benefi‐ cial for evolvability, the ease of making changes to an application.
During rolling upgrades, or for various other reasons, we must assume that different nodes are running the different versions of our application’s code. Thus, it is impor‐ tant that all data flowing around the system is encoded in a way that provides back‐ ward compatibility (new code can read old data) and forward compatibility (old code can read new data).
We discussed several data encoding formats and their compatibility properties:
• Programming language–specific encodings are restricted to a single program‐ ming language and often fail to provide forward and backward compatibility.
• Textual formats like JSON, XML, and CSV are widespread, and their compatibil‐ ity depends on how you use them. They have optional schema languages, which are sometimes helpful and sometimes a hindrance. These formats are somewhatvague about datatypes, so you have to be careful with things like numbers and binary strings.
• Binary schema–driven formats like Thrift, Protocol Buffers, and Avro allow compact, efficient encoding with clearly defined forward and backward compati‐ bility semantics. The schemas can be useful for documentation and code genera‐ tion in statically typed languages. However, they have the downside that data needs to be decoded before it is human-readable.
We also discussed several modes of dataflow, illustrating different scenarios in which data encodings are important:
• Databases, where the process writing to the database encodes the data and the process reading from the database decodes it
• RPC and REST APIs, where the client encodes a request, the server decodes the request and encodes a response, and the client finally decodes the response
• Asynchronous message passing (using message brokers or actors), where nodes communicate by sending each other messages that are encoded by the sender and decoded by the recipient
We can conclude that with a bit of care, backward/forward compatibility and rolling upgrades are quite achievable. May your application’s evolution be rapid and your deployments be frequent.