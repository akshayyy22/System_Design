Chapter 9 Summary â€“ Consistency and Consensus

ðŸŽ¯ Purpose:
This chapter dives into the core challenges of maintaining consistency in distributed systems and achieving consensus among nodes â€” essential for building reliable, fault-tolerant systems.

ðŸ“š Key Concepts:

1. Consistency Models
Defines how a distributed system ensures all nodes see the same data.


Linearizability (Strong Consistency):


Operations appear to happen in a single, globally consistent order.


Hard to scale; used where correctness is critical (e.g., leader election, locks).


Sequential Consistency:


Operations are ordered per client, but not necessarily real-time consistent.


Causal Consistency:


Preserves cause-effect relationships. Weaker but more scalable.


Eventual Consistency:


Updates propagate asynchronously. All replicas converge over time.


2. Distributed Consensus
Nodes must agree on a single value despite failures and delays.


Key Problems:
Crash Faults (node failures).


Network Partitions (nodes canâ€™t communicate).


Byzantine Faults (malicious or arbitrary failures â€” not covered in depth here).


3. Consensus Algorithms
Solve the agreement problem in unreliable networks.


Popular Algorithms:
Paxos:


Theoretically elegant but hard to implement.


Defines roles: proposer, acceptor, and learner.


Works in two phases to achieve consensus.


Raft:


Easier to understand and implement.


Used in etcd, Consul, and other real-world systems.


Relies on a leader to manage log replication.


Viewstamped Replication:


Like Paxos, but leader-based.


4. Quorums
A majority subset of nodes used to ensure overlap.


Ensures at least one node in any two quorums has the latest value.


5. Leader Election
One node is elected as the "leader" to coordinate actions.


Must ensure only one leader is active at a time to avoid split-brain issues.


6. CAP Theorem Recap
Consistency, Availability, and Partition Tolerance â€” you can pick 2 in any distributed system.


Emphasizes trade-offs when network partitions happen.


7. Consensus in Practice
Real systems (e.g., ZooKeeper, etcd) use consensus for coordination, metadata, and locks.


State machines: systems replicate a deterministic state machine via consensus.



ðŸ§  Keywords
Linearizability


Causal Consistency


Sequential Consistency


Eventual Consistency


Distributed Consensus


Paxos


Raft


Quorum


Leader Election


Split Brain


Viewstamped Replication


State Machine Replication


CAP Theorem


Fault Tolerance


Coordination Services (e.g., ZooKeeper, etcd)



ðŸ’¬ Interview Questions & Answers (Product-Based Focus)
Q1: What is the difference between linearizability and eventual consistency?
A:
 Linearizability ensures every read returns the latest written value in real-time (strong consistency).
 Eventual consistency allows temporary inconsistencies but guarantees convergence over time.
 Linearizability is suitable for critical operations like leader election, while eventual consistency is used in systems needing high availability and partition tolerance (like DNS, Amazon Dynamo).

Q2: How does the Raft algorithm ensure consensus?
A:
 Raft elects a leader who manages the log of operations. All writes go through the leader and are replicated to followers. If the leader crashes, a new leader is elected. Raft divides consensus into 3 subproblems: leader election, log replication, and safety.

Q3: Explain quorum-based writes and reads.
A:
 A quorum is a majority of nodes. For example, in a 5-node cluster, a quorum is 3.
 To ensure consistency:
A write must be acknowledged by at least 3 nodes.


A read must read from at least 3 nodes.
 The overlap ensures at least one node has the most recent write.



Q4: Why is consensus hard in asynchronous networks?
A:
 Because you can't distinguish between a slow node and a failed one. This ambiguity (known as the FLP impossibility result) makes it impossible to guarantee consensus with certainty in an asynchronous system with even one faulty node.

Q5: How do distributed systems avoid split-brain scenarios?
A:
 By using quorum-based voting and leader leases. Only one leader is allowed to make decisions at any given time, and systems like ZooKeeper or etcd enforce this via consensus protocols to avoid having multiple active leaders.

Q6: Whatâ€™s the difference between Paxos and Raft?
A:
 Both solve distributed consensus:
Paxos is theoretically robust but complex.


Raft simplifies understanding via leader-based replication and clear phases (used more widely in modern systems).



Q7: What is the role of ZooKeeper in distributed systems?
A:
 ZooKeeper provides a centralized service for configuration, naming, distributed synchronization, and group services using consensus protocols (ZAB). It ensures coordination among nodes, leader election, and metadata consistency.

Q8: Can you explain the CAP theorem in simple terms?
A:
 You can only have two out of the three:
Consistency (every node sees the same data)


Availability (system responds to requests)


Partition Tolerance (continues to work despite network splits)
 In practice, during a partition, systems must choose between being consistent or available.



If you want diagrams, summaries as flashcards, or system design applications from this chapter (like how Raft is used in Kubernetes or ZooKeeper in Hadoop), I can create those too.

